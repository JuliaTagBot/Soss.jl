var documenterSearchIndex = {"docs":
[{"location":"misc/#Models-and-JointDistributions-1","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"","category":"section"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"A Model in Soss","category":"page"},{"location":"misc/#Model-Combinators-1","page":"Models and JointDistributions","title":"Model Combinators","text":"","category":"section"},{"location":"misc/#Building-Inference-Algorithms-1","page":"Models and JointDistributions","title":"Building Inference Algorithms","text":"","category":"section"},{"location":"misc/#Inference-Primitives-1","page":"Models and JointDistributions","title":"Inference Primitives","text":"","category":"section"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"At its core, Soss is about source code generation. Instances of this are referred to as inference primitives, or simply \"primitives\". As a general rule, new primitives are rarely needed. A wide variety of inference algorithms can be built using what's provided. ","category":"page"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"To easily find all available inference primitives, enter Soss.source<TAB> at a REPL. Currently this returns this result:","category":"page"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"julia> Soss.source\nsourceLogpdf         sourceRand            sourceXform\nsourceParticles      sourceWeightedSample","category":"page"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"The general pattern is that a primitive sourceFoo specifies how code is generated for an inference function foo. ","category":"page"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"For more details on inference primitives, see the Internals section.","category":"page"},{"location":"misc/#Inference-Functions-1","page":"Models and JointDistributions","title":"Inference Functions","text":"","category":"section"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"An inference function is a function that takes a JointDistribution as an argument, and calls at least one inference primitive (not necessarily directly). The wrapper around each primitive is a special case of this, but most inference functions work at a higher level of abstraction.","category":"page"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"There's some variability , but is often of the form","category":"page"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"foo(d::JointDistribution, data::NamedTuple)","category":"page"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"For example, advancedHMC uses TuringLang/AdvancedHMC.jl , which needs a logpdf and its gradient. ","category":"page"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"Most inference algorithms can be expressed in terms of inference primitives. ","category":"page"},{"location":"misc/#Chain-Combinators-1","page":"Models and JointDistributions","title":"Chain Combinators","text":"","category":"section"},{"location":"misc/#-1","page":"Models and JointDistributions","title":"","text":"","category":"section"},{"location":"misc/#-2","page":"Models and JointDistributions","title":"","text":"","category":"section"},{"location":"misc/#Internals-1","page":"Models and JointDistributions","title":"Internals","text":"","category":"section"},{"location":"misc/#Models-1","page":"Models and JointDistributions","title":"Models","text":"","category":"section"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"struct Model{A,B}\n    args  :: Vector{Symbol}\n    vals  :: NamedTuple\n    dists :: NamedTuple\n    retn  :: Union{Nothing, Symbol, Expr}\nend","category":"page"},{"location":"misc/#","page":"Models and JointDistributions","title":"Models and JointDistributions","text":"function sourceWeightedSample(_data)\n    function(_m::Model)\n\n        _datakeys = getntkeys(_data)\n        proc(_m, st :: Assign)     = :($(st.x) = $(st.rhs))\n        proc(_m, st :: Return)     = nothing\n        proc(_m, st :: LineNumber) = nothing\n\n        function proc(_m, st :: Sample)\n            st.x ∈ _datakeys && return :(_ℓ += logpdf($(st.rhs), $(st.x)))\n            return :($(st.x) = rand($(st.rhs)))\n        end\n\n        vals = map(x -> Expr(:(=), x,x),variables(_m)) \n\n        wrap(kernel) = @q begin\n            _ℓ = 0.0\n            $kernel\n            \n            return (_ℓ, $(Expr(:tuple, vals...)))\n        end\n\n        buildSource(_m, proc, wrap) |> flatten\n    end\nend\n","category":"page"},{"location":"misc/#-3","page":"Models and JointDistributions","title":"","text":"","category":"section"},{"location":"#Soss.jl-1","page":"Home","title":"Soss.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Modules = [Soss]","category":"page"},{"location":"#Soss.MarkovChain","page":"Home","title":"Soss.MarkovChain","text":"MarkovChain\n\nMarkovChain(pars, step) defines a Markov Chain with global parameters pars and transition kernel step. Here, pars is a named tuple, and step is a Soss model that takes arguments (pars, state) and returns a next value containing the new pars and state.\n\nNOTE: This is experimental, and may change in the near future.\n\nmstep = @model pars,state begin\n    σ = pars.σ\n    x0 = state.x\n    x ~ Normal(x0, σ)\n    next = (pars=pars, state=(x=x,))\nend;\n\nm = @model s0 begin\n    σ ~ Exponential()\n    pars = (σ=σ,)\n    chain ~ MarkovChain(pars, mstep(pars=pars, state=s0))\nend;\n\nr = rand(m(s0=(x=2,),));\n\nfor s in Iterators.take(r.chain,3)\n    println(s)\nend\n\n# output\n\n(x = -6.596883394256064,)\n(x = 0.48200039561318864,)\n(x = -2.838556784903994,)\n\n\n\n\n\n","category":"type"},{"location":"#Soss.Do-Tuple{Model,Vararg{Any,N} where N}","page":"Home","title":"Soss.Do","text":"Do(m, xs...)\n\nReturns a model transformed by adding xs... to arguments. The remainder of the body remains the same, consistent with Judea Pearl's \"Do\" operator. Unneeded arguments are trimmed.\n\nExamples\n\nm = @model (n, k) begin\n    β ~ Gamma()\n    α ~ Gamma()\n    θ ~ Beta(α, β)\n    x ~ Binomial(n, θ)\n    z ~ Binomial(k, α / (α + β))\nend;\nDo(m, :θ)\n\n# output\n@model (n, k, θ) begin\n        β ~ Gamma()\n        α ~ Gamma()\n        x ~ Binomial(n, θ)\n        z ~ Binomial(k, α / (α + β))\n    end\n\n\n\n\n\n","category":"method"},{"location":"#Soss.advancedHMC-Union{Tuple{B}, Tuple{A}, Tuple{Soss.JointDistribution{A,B,B1,M} where M where B1,Any}, Tuple{Soss.JointDistribution{A,B,B1,M} where M where B1,Any,Any}} where B where A","page":"Home","title":"Soss.advancedHMC","text":"advancedHMC(m, data, N = 1000; n_adapts = 1000)\n\nDraw N samples from the posterior distribution of parameters defined in Soss model m, conditional on data. Samples are drawn using Hamiltonial Monte Carlo (HMC) from the advancedHMC.jl package.\n\nKeywords\n\nn_adapts = 1000: The number of interations used to set HMC parameters.\n\nReturns a tuple of length 2:\n\nSamples from the posterior distribution of parameters.  \nSample summary statistics.  \n\nExample\n\n\nusing Random\nRandom.seed!(42);\n\nm = @model x begin\n    β ~ Normal()\n    yhat = β .* x\n    y ~ For(eachindex(x)) do j\n        Normal(yhat[j], 2.0)\n    end\nend\n\nx = randn(3);\ntruth = rand(m(x=x));\n\npost = advancedHMC(m(x=x), (y=truth.y,));\nE_β = mean(post[1])[1]\n\nprintln(\"true β: \" * string(round(truth.β, digits=2)))\nprintln(\"Posterior mean β: \" * string(round(E_β, digits=2)))\n\n# output\ntrue β: -0.3\nPosterior mean β: -0.25\n\n\n\n\n\n","category":"method"},{"location":"#Soss.after-Tuple{Model,Vararg{Any,N} where N}","page":"Home","title":"Soss.after","text":"after(m::Model, xs...; strict=false)\n\nTransforms m by moving xs to arguments. If strict=true, only descendants of xs are retained in the body. Otherwise, the remaining variables in the body are unmodified. Unused arguments are trimmed.\n\npredictive(m::Model, xs...) = after(m, xs..., strict = true)\n\nDo(m::Model, xs...) = after(m, xs..., strict = false)\n\nExample\n\nm = @model (n, k) begin\n    β ~ Gamma()\n    α ~ Gamma()\n    θ ~ Beta(α, β)\n    x ~ Binomial(n, θ)\n    z ~ Binomial(k, α / (α + β))\nend;\nafter(m, :α)\n\n# output\n@model (n, k, α) begin\n        β ~ Gamma()\n        θ ~ Beta(α, β)\n        x ~ Binomial(n, θ)\n        z ~ Binomial(k, α / (α + β))\n    end\n\n\n\n\n\n","category":"method"},{"location":"#Soss.before-Tuple{Model,Vararg{Any,N} where N}","page":"Home","title":"Soss.before","text":"before(m::Model, xs...; inclusive=true, strict=true)\n\nTransforms m by retaining all ancestors of any of xs if strict=true; if strict=false, retains all variables that are not descendants of any xs. Note that adding more variables to xs cannot result in a larger model. If inclusive=true, xs is considered to be an ancestor of itself and is always included in the returned Model. Unneeded arguments are trimmed.\n\nprune(m::Model, xs...) = before(m, xs..., inclusive = false, strict = false)\n\nprior(m::Model, xs...) = before(m, xs..., inclusive = true, strict = true)\n\nExamples\n\nm = @model (n, k) begin\n    β ~ Gamma()\n    α ~ Gamma()\n    θ ~ Beta(α, β)\n    x ~ Binomial(n, θ)\n    z ~ Binomial(k, α / (α + β))\nend;\nbefore(m, :θ, inclusive = true, strict = false)\n\n# output\n@model k begin\n        β ~ Gamma()\n        α ~ Gamma()\n        θ ~ Beta(α, β)\n        z ~ Binomial(k, α / (α + β))\n    end\n\n\n\n\n\n","category":"method"},{"location":"#Soss.dynamicHMC","page":"Home","title":"Soss.dynamicHMC","text":"dynamicHMC(\n    rng::AbstractRNG,\n    m::JointDistribution,\n    _data,\n    N::Int = 1000;\n    method = logpdf,\n    ad_backend = Val(:ForwardDiff),\n    reporter = DynamicHMC.NoProgressReport(),\n    kwargs...)\n\nDraw N samples from the posterior distribution of parameters defined in Soss model m, conditional on _data. Samples are drawn using Hamiltonial Monte Carlo (HMC) from the DynamicHMC.jl package.\n\nThis function is essentially a wrapper around DynamicHMC.mcmc_with_warmup(). Arguments reporter, ad_backend DynamicHMC docs here)\n\nArguments\n\nrng: Random number generator.\nm: Soss model.\n_data: NamedTuple of data to condition on.\n\nKeyword Arguments\n\nN = 1000: Number of samples to draw.\nmethod = logpdf: How to compute the log-density. Options are logpdf (delegates to logpdf of each component) or codegen (symbolic simplification and code generation).\nad_backend = Val(:ForwardDiff): Automatic differentiation backend.\nreporter = DynamicHMC.NoProgressReport(): Specify logging during sampling. Default: do not log progress.\nkwargs: Additional keyword arguments passed to core sampling function DynamicHMC.mcmc_with_warmup().\n\nReturns an Array of Namedtuple of length N. Each entry in the array is a sample of parameters indexed by the parameter symbol.\n\nExample\n\n\nusing Random\nRandom.seed!(42);\nrng = MersenneTwister(42);\n\nm = @model x begin\n    β ~ Normal()\n    yhat = β .* x\n    y ~ For(eachindex(x)) do j\n        Normal(yhat[j], 2.0)\n    end\nend\n\nx = randn(50);\ntruth = rand(m(x=x));\n\npost = dynamicHMC(rng, m(x=x), (y=truth.y,));\nE_β = mean(getfield.(post, :β))\n\nprintln(\"true β: \" * string(round(truth.β, digits=2)))\nprintln(\"Posterior mean β: \" * string(round(E_β, digits=2)))\n\n# output\ntrue β: 0.3\nPosterior mean β: 0.47\n\n\n\n\n\n","category":"function"},{"location":"#Soss.predictive-Tuple{Model,Vararg{Any,N} where N}","page":"Home","title":"Soss.predictive","text":"predictive(m, xs...)\n\nReturns a model transformed by adding xs... to arguments with a body containing only statements that depend on xs, or statements that are depended upon by children of xs through an open path. Unneeded arguments are trimmed.\n\nExamples\n\nm = @model (n, k) begin\n    β ~ Gamma()\n    α ~ Gamma()\n    θ ~ Beta(α, β)\n    x ~ Binomial(n, θ)\n    z ~ Binomial(k, α / (α + β))\nend;\npredictive(m, :θ)\n\n# output\n@model (n, θ) begin\n        x ~ Binomial(n, θ)\n    end\n\n\n\n\n\n","category":"method"},{"location":"#Soss.prior-Tuple{Model,Vararg{Any,N} where N}","page":"Home","title":"Soss.prior","text":"prior(m, xs...)\n\nReturns the minimal model required to sample random variables xs.... Useful for extracting a prior distribution from a joint model m by designating xs... and the variables they depend on as the prior and hyperpriors.\n\nExample\n\nm = @model n begin\n    α ~ Gamma()\n    β ~ Gamma()\n    θ ~ Beta(α,β)\n    x ~ Binomial(n, θ)\nend;\nprior(m, :θ)\n\n# output\n@model begin\n        β ~ Gamma()\n        α ~ Gamma()\n        θ ~ Beta(α, β)\n    end\n\n\n\n\n\n","category":"method"},{"location":"#Soss.prune-Tuple{Model,Vararg{Any,N} where N}","page":"Home","title":"Soss.prune","text":"prune(m, xs...)\n\nReturns a model transformed by removing xs... and all variables that depend on xs.... Unneeded arguments are also removed.\n\nExamples\n\nm = @model n begin\n    α ~ Gamma()\n    β ~ Gamma()\n    θ ~ Beta(α,β)\n    x ~ Binomial(n, θ)\nend;\nprune(m, :θ)\n\n# output\n@model begin\n        β ~ Gamma()\n        α ~ Gamma()\n    end\n\nm = @model n begin\n    α ~ Gamma()\n    β ~ Gamma()\n    θ ~ Beta(α,β)\n    x ~ Binomial(n, θ)\nend;\nprune(m, :n)\n\n# output\n@model begin\n        β ~ Gamma()\n        α ~ Gamma()\n        θ ~ Beta(α, β)\n    end\n\n\n\n\n\n","category":"method"},{"location":"#Soss.PySymPyModule","page":"Home","title":"Soss.PySymPyModule","text":"As type encoding a PyObject is unsafe without some hard works with reference counting, we simply use a Julia proxy to imitate the sympy module, e.g.,     sympy.attr = _pysympy.attr\n\n\n\n\n\n","category":"type"}]
}
